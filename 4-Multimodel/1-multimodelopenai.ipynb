{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8834256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CLIP model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 2 docs (2 embeddings)\n",
      "‚úÖ Stored 1 images\n",
      "‚úÖ Using Ollama model: phi\n",
      "\n",
      "‚ùì Query: What does the chart on page 1 show about revenue trends?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 2 documents:\n",
      "- text (page 0)\n",
      "- image (page 0)\n",
      "\n",
      "\n",
      "üí° Answer:  The chart on page 1 shows that revenue trends show a steady increase in annual revenue across three quarters, with the highest growth recorded in Q3. Q1 showed a moderate increase as new product lines were introduced, and Q2 outperformed Q1 due to marketing campaigns. Q3 had exponential growth due to global expansion.\n",
      "\n",
      "\n",
      "‚ùì Query: Summarize the main findings from the document\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 2 documents:\n",
      "- text (page 0)\n",
      "- image (page 0)\n",
      "\n",
      "\n",
      "üí° Answer:  The user wants a summary of the annual revenue trends across Q1, Q2, and Q3 from a document. The assistant will summarize that the document shows that the revenue grew steadily with the highest growth recorded in Q3, which had exponential growth due to global expansion. There was also moderate increase in Q1 as new product lines were introduced and Q2 outperformed Q1 due to marketing campaigns.\n",
      "\n",
      "\n",
      "‚ùì Query: What visual elements are present in the document?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 2 documents:\n",
      "- text (page 0)\n",
      "- image (page 0)\n",
      "\n",
      "\n",
      "üí° Answer:  Based on the provided context, the visual elements present in the document include a chart and an image. The chart shows revenue trends across three quarters (Q1, Q2, and Q3). There is no additional information given about the specific type of chart or any other graphics used in the document. As for the image from page 0, it is not described further without context, so its content cannot be determined from the provided text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Install dependencies (run once) ---\n",
    "# !pip install --upgrade transformers huggingface_hub torch pillow scikit-learn python-dotenv pymupdf faiss-cpu langchain langchain-community\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import io, base64\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# --- Load env (not required for Ollama, but kept for compatibility) ---\n",
    "load_dotenv()\n",
    "\n",
    "# --- Initialize CLIP model ---\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_id)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
    "clip_model.eval()\n",
    "print(\"‚úÖ CLIP model loaded successfully\")\n",
    "\n",
    "# === Embedding functions ===\n",
    "def embed_image(image_data):\n",
    "    if isinstance(image_data, str):\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:\n",
    "        image = image_data\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "def embed_text(text):\n",
    "    inputs = clip_processor(\n",
    "        text=text, return_tensors=\"pt\", padding=True, truncation=True, max_length=77\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "# === Process PDF ===\n",
    "pdf_path = \"multimodal_sample.pdf\"   # change to your PDF\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "all_docs, all_embeddings, image_data_store = [], [], {}\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "for i, page in enumerate(doc):\n",
    "    # --- Process Text ---\n",
    "    text = page.get_text()\n",
    "    if text.strip():\n",
    "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "        text_chunks = splitter.split_documents([temp_doc])\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "\n",
    "    # --- Process Images ---\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "\n",
    "            # Save as base64\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "\n",
    "doc.close()\n",
    "print(f\"‚úÖ Processed {len(all_docs)} docs ({len(all_embeddings)} embeddings)\")\n",
    "print(f\"‚úÖ Stored {len(image_data_store)} images\")\n",
    "\n",
    "# === Create FAISS index ===\n",
    "embeddings_array = np.array(all_embeddings)\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
    "    embedding=None,\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")\n",
    "\n",
    "# === Use Ollama (phi as default, fallback to mistral:q4_0) ===\n",
    "try:\n",
    "    llm = ChatOllama(model=\"phi\")\n",
    "    print(\"‚úÖ Using Ollama model: phi\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Could not load phi, trying mistral:q4_0\")\n",
    "    llm = ChatOllama(model=\"mistral:q4_0\")\n",
    "\n",
    "# === Retrieval ===\n",
    "def retrieve_multimodal(query, k=5):\n",
    "    query_embedding = embed_text(query)\n",
    "    results = vector_store.similarity_search_by_vector(embedding=query_embedding, k=k)\n",
    "    return results\n",
    "\n",
    "# === Create multimodal message ===\n",
    "def create_multimodal_message(query, retrieved_docs):\n",
    "    content = []\n",
    "    content.append({\"type\": \"text\", \"text\": f\"Question: {query}\\n\\nContext:\\n\"})\n",
    "\n",
    "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "\n",
    "    if text_docs:\n",
    "        text_context = \"\\n\\n\".join([\n",
    "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\" for doc in text_docs\n",
    "        ])\n",
    "        content.append({\"type\": \"text\", \"text\": f\"Text excerpts:\\n{text_context}\\n\"})\n",
    "\n",
    "    for doc in image_docs:\n",
    "        image_id = doc.metadata.get(\"image_id\")\n",
    "        if image_id in image_data_store:\n",
    "            content.append({\"type\": \"text\", \"text\": f\"[Image from page {doc.metadata['page']}] included\"})\n",
    "\n",
    "    content.append({\"type\": \"text\", \"text\": \"Please answer based on the provided context.\"})\n",
    "    return HumanMessage(content=content)\n",
    "\n",
    "# === Main pipeline ===\n",
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    retrieved = retrieve_multimodal(query, k=5)\n",
    "    message = create_multimodal_message(query, retrieved)\n",
    "    response = llm.invoke([message])\n",
    "\n",
    "    print(f\"\\nRetrieved {len(retrieved)} documents:\")\n",
    "    for doc in retrieved:\n",
    "        print(f\"- {doc.metadata.get('type')} (page {doc.metadata.get('page')})\")\n",
    "    print(\"\\n\")\n",
    "    return response.content\n",
    "\n",
    "# === Run queries ===\n",
    "if __name__ == \"__main__\":\n",
    "    queries = [\n",
    "        \"What does the chart on page 1 show about revenue trends?\",\n",
    "        \"Summarize the main findings from the document\",\n",
    "        \"What visual elements are present in the document?\"\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        print(f\"\\n‚ùì Query: {q}\")\n",
    "        print(\"-\" * 50)\n",
    "        ans = multimodal_pdf_rag_pipeline(q)\n",
    "        print(f\"üí° Answer: {ans}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
