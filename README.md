# ğŸ“š Multimodal PDF Q&A

An **AI-powered application** that lets you upload a PDF and ask questions about both **text and images inside the document**.  
It uses **CLIP embeddings**, **LangChain**, **FAISS**, and **Ollama LLMs** to retrieve multimodal context and generate answers.  
The interface is built with **Gradio** for an easy-to-use web UI.  

---

## ğŸš€ Features  
- ğŸ” Extracts **text and images** from PDF documents  
- ğŸ–¼ï¸ Embeds both **text & images** with CLIP  
- ğŸ“– Splits text into manageable chunks for better retrieval  
- ğŸ“Š Stores embeddings in **FAISS vector database**  
- ğŸ’¬ Answers questions using **Ollama LLMs** (phi / mistral)  
- ğŸŒ Interactive **Gradio UI**  

---

## ğŸ› ï¸ Installation  

### 1ï¸âƒ£ Clone the repository  
```bash
git clone https://github.com/your-username/multimodal-pdf-qa.git
cd multimodal-pdf-qa
```

### 2ï¸âƒ£ Create a virtual environment  
```bash
python -m venv venv
source venv/bin/activate   # On Linux/Mac
venv\Scripts\activate    # On Windows
```

### 3ï¸âƒ£ Install dependencies  
```bash
pip install -r requirements.txt
```

Or manually:  
```bash
pip install gradio transformers huggingface_hub torch pillow scikit-learn python-dotenv pymupdf faiss-cpu langchain langchain-community
```

### 4ï¸âƒ£ Setup environment variables  
Create a `.env` file in the project root if needed for API keys or custom configs. Example:  
```
OLLAMA_HOST=http://localhost:11434
```

---

## â–¶ï¸ Usage  

Run the app:  
```bash
python app.py
```

This will launch a **local Gradio UI** in your browser.  

1. ğŸ“„ Upload a PDF file  
2. â“ Ask a question about the content or images inside  
3. ğŸ¤– Get context-aware answers powered by LLM  

---

## ğŸ“‚ Project Structure  
```
multimodal-pdf-qa/
â”‚â”€â”€ app.py                # Main application script
â”‚â”€â”€ requirements.txt      # Python dependencies
â”‚â”€â”€ .env                  # Environment variables (optional)
â”‚â”€â”€ README.md             # Project documentation
```

---

## âš¡ Example Workflow  
1. Upload a research paper or report PDF.  
2. Ask: *"What does the chart on page 3 represent?"*  
3. The system retrieves text + image embeddings.  
4. The LLM provides a context-aware answer.  

---

## ğŸ§° Tech Stack  
- **Python**  
- **Gradio** (UI)  
- **Transformers (CLIP)** (embeddings)  
- **FAISS** (vector search)  
- **LangChain** (document & LLM integration)  
- **Ollama** (LLM inference)  

---

## ğŸ“Œ Notes  
- Requires [Ollama](https://ollama.ai) installed locally to run LLM models like `phi` or `mistral`.  
- Works best on PDFs containing both text and images.  
- Large PDFs may take time to process.  

---

## ğŸ—ï¸ Future Improvements  
- Support for **multi-page image previews**  
- Add **chat history memory**  
- Support for **more embedding models**  
- Deploy on **Streamlit / Hugging Face Spaces**  

---

## ğŸ“œ License  
This project is licensed under the **MIT License**.  
